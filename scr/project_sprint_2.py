# -*- coding: utf-8 -*-
"""Project Sprint 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YPcG9YniT-HwqMK0xmqObJPV34sBXams

# **Project Introduction: Credit Scoring Analysis**

In this project, the task is to prepare a report for the credit division of a bank focusing on the impact of customers' marital status and number of children on the probability of defaulting on loans. The bank has provided extensive data on customer creditworthiness. Credit assessment is vital for evaluating the ability of borrowers to repay their loans.

**Step of Project**

1. Data Exploration
2. Data Preprocessing
3. Data Categorization:
4. Conclusion

Data Description

1. children: Number of children in the family.
2. days_employed: Duration of employment.
3. dob_years: Age of the customer.
4. education: Level of education.
5. education_id: Identifier for education level.
6. family_status: Marital status.
7. family_status_id: Identifier for marital status.
8. gender: Gender of the customer.
9. income_type: Type of income.
10. debt: Whether the customer has ever defaulted on a loan.
11. total_income: Monthly income.
12. purpose: Purpose of the loan.

This project aims to provide insights into factors influencing credit risk using real-world data that may contain outdated information or inaccuracies, reflecting common challenges in data management scenarios.

## **Data Exploration**
"""

import pandas as pd

dataset = pd.read_csv('/content/credit_scoring_eng.csv')

dataset.shape

dataset.head(5)

dataset.info()

"""Based on the info shown above, all values are filled in the data, so we do not have any problem with it. But we can see an obstacle that might need to be sorted out during this project: There are missing value in the column 'days_employed' and 'total_income', we might will solve the problem by filling the missing value using mean or median of filled value.

Now let us see what are those.
"""

dataset.isna().sum()

"""We can see the first column that contains missing value is 'days_employed' the followed by 'total_income'. Then lets see if its semitrical or not by displaying the data with missing value."""

dataset.loc[dataset['days_employed'].isna()]

"""We can see the missing value resemble each other between 'days_employed' and 'total_income'. So we can assume that the missing value is symetrical."""

dataset_not_null = dataset.dropna()
dataset_not_null

"""There is some missing value in column 'days_employed' and 'total_income' thus we will manipulate the data in order to give clear output. we can use methode as bellow:

1. Fill the missing value with median or mean value of filled values in both columns.
2. Median or mean can be from category of certain column that we will find out later.
3. Once missing value in both column are filled, we can continue the project to analyze the data.
"""

dataset_not_null = dataset[dataset['days_employed'].isnull()]

dataset_not_null.describe()

"""We can see the missing value are symetrical already, and now lets find out the closest result to fill the missing value.

The possibility of jumbled missing value is low since the data missing is symetrical, but lets find out.
"""

dataset['days_employed'].value_counts(normalize=True)

dataset['days_employed'].describe()

401755.400475/365

dataset['total_income'].value_counts(normalize=True)

dataset['family_status'].unique()

dataset['income_type'].unique()

dataset['dob_years'].unique()

dataset.info()

dataset.groupby('education')['total_income'].median()

"""The data that missing is from column 'days_employed' and 'total_income', the anomalities from other column may found in the next step that is transformation data

## **Data Processing**

Lets transform the data as method bellow:

1. Make sure the spelling is standardize.
2. Make sure there is no duplicate value.
3. Make sure the wrong input is replaced with better value.
4. Categorize data in order to make easier analysis.
"""

dataset.groupby('education').count()

dataset['education'] = dataset['education'].replace({
    "BACHELOR'S DEGREE": "bachelor degree",
    "Bachelor's Degree": "bachelor degree",
    "bachelor's degree": "bachelor degree",
    "GRADUATE DEGREE": "graduate degree",
    "graduate degree": "graduate degree",
    "PRIMARY EDUCATION": "primary education",
    "SECONDARY EDUCATION": "secondary education",
    "SOME COLLEGE": "some college",
    "some college": "some college",
    "primary education": "primary education",
    "secondary education": "secondary education",
    "Secondary Education": "secondary education",
    "Some College": "some college",
    "Primary Education": "primary education",
    "Graduate Degree": "graduate degree"
})

dataset['education'].unique()

dataset['children'].value_counts()

"""There some anomalies in number of children. Above shown that the spread is 0 - 5 with anomalies of value -1 and 20. We can say that because there is no value between 5 to 20 and there is no way the number is bellow 0. There is a possibility -1 and 20 are typos when they are supposed to be 1 and 2."""

dataset['children'] = dataset['children'].replace({
    -1:1,
    20:2
})

dataset['children'].value_counts()

days_employed_all = dataset['days_employed'].count()
days_employed_missing = dataset['days_employed'].isna().sum()

percentage_missing = (days_employed_missing / days_employed_all) * 100


print(days_employed_all)
print(days_employed_missing)
print(percentage_missing)

"""In this case we will fill missing value in days_employed using the mean value."""

dataset.groupby('income_type')['days_employed'].min()

median_days_employed = dataset['days_employed'].median()
dataset['days_employed'] = dataset['days_employed'].fillna(median_days_employed)

dataset.isna().sum()

dataset.groupby('dob_years').count()

"""Seems like there is 0 value in the data in column 'dob_years' which is impossible. This may caused by wrong input."""

filtered_data = dataset[dataset['dob_years'] == 0]
filtered_data.head(5)

zero_count = dataset['dob_years'].eq(0).sum()
non_zero_count = dataset['dob_years'].ne(0).sum()
total_count = zero_count + non_zero_count

zero_percentage = (zero_count / total_count)
non_zero_percentage = (non_zero_count / total_count)


print(zero_count)
print(non_zero_count)
print(zero_percentage)

"""Based on analysis above, there is no pattern on why the the value in 'dob_years' is 0. And since the percentage is not that much (only 0.4%), we better erase them in order to have clear data"""

avg_age = round(dataset['dob_years'].mean())
avg_age

dataset.loc[dataset['dob_years'] == 0, 'dob_years'] = avg_age

unique_dob_years = dataset['dob_years'].unique()
unique_dob_years.sort()
print(unique_dob_years)

dataset['family_status'].unique()

dataset.groupby('family_status')['children'].mean()

dataset.groupby('family_status')['total_income'].mean()

dataset.groupby('family_status').count()

dataset['family_status'] = dataset['family_status'].replace({
   "civil partnership": "married" })

dataset['family_status'].unique()

"""Actually beside synonym I do not see any problem with "family_status" column."""

dataset['gender'].unique()

gender_counts = dataset['gender'].value_counts()
gender_counts

"""Since its only 1 XNA, we can remove it as well"""

dataset = dataset[dataset['gender'] != 'XNA']

gender_counts = dataset['gender'].value_counts()
gender_counts

income = dataset['income_type'].value_counts()
income

filtered_income = dataset[dataset['income_type'].isin(['unemployed', 'student'])]
filtered_income

"""We can see there is an anomali where source income from unemployment and student. This is unreasonable since those two is not defining where the money come from."""

dataset = dataset[~dataset['income_type'].isin(['unemployed', 'student'])]

income = dataset['income_type'].value_counts()
income

"""Now lets see if there is any duplicate values."""

duplicate_count = dataset.duplicated().sum()
duplicate_count

"""There is 71 duplicated data in dataset"""

dataset = dataset.drop_duplicates()

duplicate_count = dataset.duplicated().sum()
duplicate_count

dataset.info()

"""We can see after the cleaning process the value of row is decreased but the data is clear already, you can see different number in total_income because we assumed not all applicant have income. Now lets handle the missing value."""

dataset[['days_employed', 'total_income']].describe()

"""### **Handling missing value in total_income**

As stated before there is also missing value in 'total_income' therefore we are going to fill the missing value using mean of filled value for each category. The category are as bellow:
"""

def age_grouping(age):
    if age <= 20:
        value = '11 - 20 years old'
    elif age <= 30:
        value = '21 - 30 years old'
    elif age <= 40:
        value = '31 - 40 years old'
    elif age <= 50:
        value = '41 - 50 years old'
    elif age <= 60:
        value = '51 - 60 years old'
    elif age <= 70:
        value = '61 - 70 years old'
    else:
        value = '>70 years old'
    return value

age_grouping(19)

dataset['age_grouping'] = dataset['dob_years'].apply(age_grouping)

dataset['age_grouping'].value_counts()

"""Age group is used because the possibility of higher age group earn more money."""

filtered_dataset = dataset.dropna(subset=['total_income'])
filtered_dataset.info()

filtered_dataset.groupby('age_grouping')['total_income'].mean()

age_income = pd.pivot_table(filtered_dataset, index='age_grouping', values='total_income', aggfunc='median').reset_index()
age_income

def replace_income(data, grouping):
    index = 0
    for group in grouping['age_grouping']:
        data.loc[(data['age_grouping'] == group) & (data['total_income'].isnull()), 'total_income'] = grouping.iloc[index, 1]
        index = index + 1
    return data

dataset = replace_income(data=dataset, grouping=age_income)

dataset.isnull().sum()

dataset.info()

"""Seems like everything is in order now.

### **Handling missing value in days_employed**

As stated above, missing value also occur in 'days_employed', but the other problem is the value is doesnt make sense since there is negative value and number that doesnt make sense. So these are the next step:

Fill the missing value with mean/median. (its already done but we gave you insight again bellow)
Absolut the negative values.
Fill the anomalies value with our own based determination.
"""

dataset['days_employed'] = abs(dataset['days_employed'])

dataset['years_employed'] = round(dataset['days_employed']/365)

median = pd.pivot_table(dataset, index='age_grouping', values='years_employed', aggfunc='median').reset_index()
median

mean = pd.pivot_table(dataset, index='age_grouping', values='years_employed', aggfunc='mean').reset_index()
mean

dataset['years_employed'].describe()

"""In this case we will use median since there is outliers. But the years_employed doesnt make any sense, so we are gonna fix it."""

dataset['years_working'] = dataset['dob_years'] - 18
dataset.head(5)

dataset.loc[dataset['years_employed'] > dataset['years_working'], 'days_employed'] = dataset['years_working'] * 365

dataset['days_employed'].describe()

dataset['years_employed'] = round(dataset['days_employed']/365)

dataset['years_employed'].describe()

mean = pd.pivot_table(dataset, index='age_grouping', values='days_employed', aggfunc='mean').reset_index()
mean

dataset.info()

dataset['purpose'].unique()

dataset['purpose'] = dataset['purpose'].replace({
    "purchase of the house": "House Ownership",
    "car purchase": "Vehicle Ownership",
    "supplementary education": "Education",
    "having a wedding": "Wedding",
    "housing transactions": "House Ownership",
    "education": "Education",
    "to have a wedding": "Wedding",
    "purchase of the house for my family": "House Ownership",
    "buy real estate": "House Ownership",
    "buy commercial real estate": "House Ownership",
    "buy residential real estate": "House Ownership",
    "construction of own property": "House Construction",
    "property": "House Ownership",
    "building a property": "House Construction",
    "buying a second-hand car": "Vehicle Ownership",
    "buying my own car": "Vehicle Ownership",
    "transactions with commercial real estate": "House Ownership",
    "building a real estate": "House Ownership",
    "housing": "House Ownership",
    "transactions with my real estate": "House Ownership",
    "cars": "Vehicle Ownership",
    "to become educated": "Education",
    "second-hand car purchase": "Vehicle Ownership",
    "getting an education": "Education",
    "car": "Vehicle Ownership",
    "wedding ceremony": "Wedding",
    "to get a supplementary education": "Education",
    "purchase of my own house": "House Ownership",
    "real estate transactions": "House Ownership",
    "getting higher education": "Education",
    "to own a car": "Vehicle Ownership",
    "purchase of a car": "Vehicle Ownership",
    "profile education": "Education",
    "university education": "Education",
    "buying property for renting out": "House Ownership",
    "to buy a car": "Vehicle Ownership",
    "housing renovation": "House Construction",
    "going to university": "Education",
})

dataset['purpose'].unique()

"""## **Data Categorization**

The next step is categorize the data, lets explore what we have. In this case we will try purpose first, but theres open possibilities to explore another column.
"""

dataset.groupby('purpose').count()

dataset['purpose'].unique()

"""lets categorize the income level so its easier to analyze."""

def income_level(income):
    if income <= 10000:
        return 'Small'
    elif income <= 20000:
        return 'Average'
    elif income <= 25000:
        return 'Above Average'
    elif income <= 50000:
        return 'High'
    elif income > 50000:
        return 'Very High'
    else:
        return 'Unknown'

dataset['income_category'] = dataset['total_income'].apply(income_level)
dataset.head(5)

"""Let us as well categorize the numeric data."""

dataset['purpose'].value_counts()

dataset['age_grouping'].value_counts()

dataset['children'].value_counts()

dataset['debt'].value_counts()

dataset.describe()

def number_children(children):
    if children == 0:
        return 'Childless'
    elif children == 1:
        return 'Single Child'
    elif children == 2:
        return 'Average'
    elif children <= 4:
        return 'Big Family'
    elif children >= 5:
        return 'Very Big Family'
    else:
        return 'Unknown'

dataset['family_category'] = dataset['children'].apply(number_children)

dataset['family_category'].value_counts()

"""## **Checking Hypothesis**

**Is there a correlation between having children and the probability of loan default?**
"""

df_tmp = pd.crosstab(dataset['children'], dataset['debt'])
df_tmp['bad_rate'] = df_tmp[1]*100/df_tmp.sum(axis=1)
df_tmp

debt_by_children = dataset.loc[dataset['debt'] == 1]
debt_by_children

debt_by_children['debt'].unique()

children_vs_debt = pd.pivot_table(debt_by_children, index=['family_category'], columns='debt', values='children', aggfunc='mean')

total_counts = children_vs_debt.sum().sum()
children_vs_debt_percentage = children_vs_debt / total_counts * 100
children_vs_debt_percentage

debt_by_children_0 = dataset.loc[dataset['debt'] == 0]
debt_by_children_0

children_vs_debt = pd.pivot_table(debt_by_children_0, index='family_category', columns='debt', values='children', aggfunc='mean')

total_counts = children_vs_debt.sum().sum()
children_vs_debt_percentage = children_vs_debt / total_counts * 100
children_vs_debt_percentage

df_tmp_0 = pd.crosstab(dataset['family_category'], dataset['debt'])
df_tmp_0

df_tmp_0['bad_rate'] = df_tmp_0[1]*100/df_tmp_0.sum(axis=1)
df_tmp_0

"""Conclusion

Based on calculation we can see the the highest bad debt happened to family that having big number of family member with staggering 51% of bad debt case. Which is make sense since bigger family needs bigger home, bigger car, as well another daily needs.

But based on the percentage within category, the highest probability for bad debt is average family with 9.4%

**Is there a correlation between marital status and the probability of loan default?**
"""

debt_by_family = dataset.loc[dataset['debt'] == 1]
debt_by_family

family_vs_debt = pd.pivot_table(debt_by_family, index=['family_status'], columns='debt', values='children', aggfunc='mean')

family_total_counts = family_vs_debt.sum().sum()
family_vs_debt_percentage = family_vs_debt / family_total_counts * 100
family_vs_debt_percentage

family_vs_debt = pd.pivot_table(debt_by_family, index=['family_status', 'purpose'], columns='debt', values='children', aggfunc='mean')

family_total_counts = family_vs_debt.sum().sum()
family_vs_debt_percentage = family_vs_debt / family_total_counts * 100
family_vs_debt_percentage

df_tmp = pd.crosstab(dataset['family_status'], dataset['debt'])
df_tmp.head()

df_tmp['bad_rate'] = df_tmp[1]*100/df_tmp.sum(axis=1)
df_tmp

"""Conclusion

Based on calculation we can see that married couple has highest number percentage of bad debt with 38.9% out of all category. This is make sense when become a family they needs home & car of their own. But if we see spesifically by status category among its success and failed, we can see that unmarried couple has the highest percentage of possibility of bad debt with 9.7& possibility and followed by married with 7.9%.

So by total loan given the highest percentage of bad debt is married couple with 38.9% of total bad debt, but if we calculate the percentage within category unmarried couple has the highest percentage with 9.7%

**Is there a correlation between income level and the probability of loan default?**
"""

debt_earning = dataset.loc[dataset['debt'] == 1]

earning_vs_debt = pd.pivot_table(debt_earning, index='income_category', columns='debt', values='children', aggfunc='mean')

earning_total_counts = earning_vs_debt.sum().sum()
earning_vs_debt_percentage = earning_vs_debt / earning_total_counts * 100
earning_vs_debt_percentage

df_tmp_2 = pd.crosstab(dataset['income_category'], dataset['debt'])
df_tmp_2.head()

df_tmp_2['bad_rate'] = df_tmp_2[1]*100/df_tmp_2.sum(axis=1)
df_tmp_2

"""Conclusion

Based on the calculation its actually vary if we see the cause of bad debt using income category which only have different approximately 1% from each category. But the highest is when the income categorize as very high with range above 50000. And if we calculate percentage within each income type the percentage shows the same range possibility, but the highest possibility land on above average and average with bracket of 10000 to 25000 with percentage of 8.6% and 8.4%.

So by percentage of total loan the category of income very high has the vast amount of loan with 20.8% in total but by its possibility family with above average and average income has more possibility with 8.6% and 8.4% of bad debt possibility.

**How does the loan purpose affect the default rate?**
"""

debt_purpose = dataset.loc[dataset['debt'] == 1]

purpose_vs_debt = pd.pivot_table(debt_purpose, index='purpose', columns='debt', values='children', aggfunc='mean')
purpose_total_counts = purpose_vs_debt.sum().sum()
purpose_vs_debt_percentage = purpose_vs_debt / purpose_total_counts * 100
purpose_vs_debt_percentage

df_tmp_3 = pd.crosstab(dataset['purpose'], dataset['debt'])
df_tmp_3.head()

df_tmp_3['bad_rate'] = df_tmp_3[1]*100/df_tmp_3.sum(axis=1)
df_tmp_3

"""Conclusion

Based on the calculation we can see the highest possibility is home construction with 23.3% and followed by education, home ownership, vehicle ownership, and wedding with the range from 18.5% to 19.91%. Home construction is at risk because the estimated cost can overrun then if its use for productive asset cost overrun will affect the repayment of debt.

By calculate the percentage within each category of purpose, we discovered new fact that the homw ownership and home construction placed bottom, the highest still car ownership with 9.3% and education with 9.2%.
"""

debt_all = dataset.loc[dataset['debt'] == 1]

purpose_children_vs_debt = pd.pivot_table(debt_all, index=['family_category','purpose'], columns='debt', values='children', aggfunc='mean')
purpose_children_total_counts = purpose_children_vs_debt.sum().sum()
purpose_children_vs_debt_percentage = purpose_children_vs_debt / purpose_children_total_counts * 100
purpose_children_vs_debt_percentage

purpose_children_vs_debt = pd.pivot_table(dataset, index=['family_category', 'purpose'], columns='debt', values='children', aggfunc='count').reset_index()
purpose_children_vs_debt

# Convert the 'debt' columns to numeric type
purpose_children_vs_debt[1] = pd.to_numeric(purpose_children_vs_debt[1], errors='coerce')
purpose_children_vs_debt[0] = pd.to_numeric(purpose_children_vs_debt[0], errors='coerce')

# Calculate the 'bad_rate' excluding non-numeric columns
purpose_children_vs_debt['bad_rate'] = purpose_children_vs_debt[1] * 100 / (purpose_children_vs_debt[0] + purpose_children_vs_debt[1])

purpose_children_vs_debt.sort_values('bad_rate', ascending=False)

"""Based on the data, significant variations in default rates are observed across different family categories and loan purposes. Big families show the highest default rates, particularly for weddings (14.29%) and house construction (12.50%), suggesting higher financial strain in these contexts. Average families also exhibit notable default risks, notably for vehicle ownership (12.05%) and education (11.24%) loans. Single-child families display moderate default rates ranging from 7.86% to 10.68%, while childless families generally show lower default rates, ranging from 5.87% to 8.67%. Interestingly, very big families show no defaults across all recorded purposes, which may reflect a smaller sample size or unique financial stability. These findings underline the importance of family dynamics and loan purpose in assessing credit risk and informing lending decisions.

## **Summary**

In this project given the data of historical performance of creditors and its following information. But during data understanding we found that:

1. Missing value found in column 'days_employed' and 'total_income'.
2. In 'days_employed' column has unrealistic value.
3. Redundant in column 'purpose'.
4. Unrealistic value in column 'children'.
5. Uncategorized data.

So some adjustmen has been made.

1. Using average to filled missing value in 'days_employed' and 'total_income'. for column 'days_employed' we also using absolut since there is negation value and also using realistic value to fill unrealistic value. Such as we substract the column 'dob_years' with 18 (the start age of legally working) in order to replace the unrealistic value. And for column 'total_income' we categrized first based on 'age_category' so the result is closed to relevant, then filled the missing value with the average value from each age category.
2. Redundant value has been replaced in column purpose with replace function.
3. Unrealistic value in column children has been replaced with realistic value (-1) to (1) and (20) to (2). Since there is no way having negation children and spread is only until 5 for top value.
4. Categorized data in column 'income', 'children', 'days_employed'.

The conclusion we can give from the data above is

1. Big families demonstrate the highest default rates, notably for weddings (14.29%) and house construction (12.50%), indicating higher financial strain in these scenarios.
2. Average families show significant default risks, particularly for vehicle ownership (12.05%) and education (11.24%) loans.
3. Single-child families exhibit moderate default rates ranging from 7.86% to 10.68% across different loan purposes.
4. Childless families generally exhibit lower default rates, ranging from 5.87% to 8.67%.
5. Very big families show no defaults across all recorded purposes, possibly due to a smaller sample size or unique financial stability.

These findings underscore the critical role of family dynamics and loan purpose in evaluating credit risk and informing prudent lending practices.
"""

